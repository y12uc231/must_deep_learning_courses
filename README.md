# Must Deep (Machine) learning courses
Hi, I am Satya and I graduated from Carnegie Mellon University in 2018. This page has links to some helpful courses that helped me in learning the fundamentals and advanced concepts of machine learning and deep learning. 

Three principles that worked for me so far :-
1. DO the math, dont just read it. 
2. CODE the math, dont just do it on paper. 
3. APPY the math to a different problem, dont just code it. 

[CS229](http://cs229.stanford.edu/) : Machine Learning from Stanford (Andrew Ng)

This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.

[10-601](http://www.cs.cmu.edu/~mgormley/courses/10601/) Machine Learning 101 course from CMU (Multiple awesome faculties)

Machine Learning is concerned with computer programs that automatically improve their performance through experience (e.g., programs that learn to recognize human faces, recommend music and movies, and drive autonomous robots). This course covers the theory and practical algorithms for machine learning from a variety of perspectives. We cover topics such as Bayesian networks, decision tree learning, Support Vector Machines, statistical learning methods, unsupervised learning and reinforcement learning. The course covers theoretical concepts such as inductive bias, the PAC learning framework, Bayesian learning methods, margin-based learning, and Occam’s Razor. Programming assignments include hands-on experiments with various learning algorithms. This course is designed to give a graduate-level student a thorough grounding in the methodologies, technologies, mathematics and algorithms currently needed by people who do research in machine learning.

[10-701](http://www.cs.cmu.edu/~lwehbe/10701_S20/) Advanced Machine Learning from CMU (Taught by multiple ML professors)

Machine learning studies the question "How can we build computer programs that automatically improve their performance through experience?" This includes learning to perform many types of tasks based on many types of experience. For example, it includes robots learning to better navigate based on experience gained by roaming their environments, medical decision aids that learn to predict which therapies work best for which diseases based on data mining of historical health records, and speech recognition systems that learn to better understand your speech based on experience listening to you. This course is designed to give PhD students a thorough grounding in the methods, mathematics and algorithms needed to do research and applications in machine learning. Students entering the class with a pre-existing working knowledge of probability, statistics and algorithms will be at an advantage, but the class has been designed so that anyone with a strong numerate background can catch up and fully participate. If you are interested in this topic, but are not a PhD student, or are a PhD student not specializing in machine learning, you might consider the master's level course on Machine Learning, 10-601. 10-601 may be appropriate for MS and undergrad students who are interested in the theory and algorithms behind ML. You can see an ML course comparison here.

[Intro to Reinforcement Learning by David Silver](https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver) 

David silver is one of the rockstars of reinforcement learning and I strongly suggest this course since it covers fundamentals and provides a good ramp to learn advanced RL. Reinforcement Learning has emerged as a powerful technique in modern machine learning, allowing a system to learn through a process of trial and error. It has been succesfully applied in many domains, including systems such as AlphaZero, that learnt to master the games of chess, Go and Shogi. This lecture series, taught at University College London by David Silver - DeepMind Principal Scienctist, UCL professor and the co-creator of AlphaZero - will introduce students to the main methods and techniques used in RL. Students will also find Sutton and Barto’s classic book, Reinforcement Learning: an Introduction a helpful companion.

[36-705 Intermediate Statistics ](http://www.stat.cmu.edu/~larry/=stat705/) by Larry Wasserman

This course covers the fundamentals of theoretical statistics. Topics include: concentration of
measure, basic empirical process theory, convergence, point and interval estimation, maximum
likelihood, hypothesis testing, Bayesian inference, nonparametric statistics and bootstrap resampling. This course is excellent preparation for advanced work in Statistics and Machine Learning. Some course objectives for students in machine learning include: (1) Predict which kinds
of existing machine learning algorithms will be most suitable for which sorts of tasks, based
on formal properties and experimental results. (2) Evaluate and analyze existing learning
algorithms. There are several textbooks that we will use material from. The main textbook for the
course will be “All of Statistics” (Wasserman 2004). You can get the pdf through the library
for free. We will cover Chapters 1-12 from the text plus some supplementary material. 

[Deep Mind Deep Learning Lecture Series](https://deepmind.com/learning-resources/deep-learning-lecture-series-2020) 

These are lectures from the best in each algorithm of deep learning. These are my personal best to cover breadth and depth of any area of deep learning. Deep Learning has been applied to problems in object recognition, speech recognition, speech synthesis, forecasting, scientific computing, control and many more. The resulting applications are touching all of our lives in areas such as healthcare and medical research, human-computer interaction, communication, transport, conservation, manufacturing and many other fields of human endeavour. In recognition of this huge impact, the 2019 Turing Award, the highest honour in computing, was awarded to pioneers of Deep Learning.

[CS 330 : Deep Multi-Task and Meta Learning](https://cs330.stanford.edu/)

While deep learning has achieved remarkable success in supervised and reinforcement learning problems, such as image classification, speech recognition, and game playing, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:
* goal-conditioned reinforcement learning techniques that leverage the structure of the provided goal space to learn many tasks significantly faster
* meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly
* curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer

This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta learning algorithms and be ready to conduct research on these topics.

[Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)

Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. The final assignment will involve training a multi-million parameter convolutional neural network and applying it on the largest image classification dataset (ImageNet). We will focus on teaching how to set up the problem of image recognition, the learning algorithms (e.g. backpropagation), practical engineering tricks for training and fine-tuning the networks and guide the students through hands-on assignments and a final course project. Much of the background and materials of this course will be drawn from the ImageNet Challenge.


[CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)

Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.

[Analyses of Deep Learning (STATS 385)](https://stats385.github.io/readings)

This course a lot of mathematical depth of deep learning. Deep learning is a transformative technology that has delivered impressive improvements in image classification and speech recognition. Many researchers are trying to better understand how to improve prediction performance and also how to improve training methods. Some researchers use experimental techniques; others use theoretical approaches. In this course we will review both experimental and theoretical analyses of deep learning. We will have 8 guest lecturers as well as graded projects for those who take the course for credit.

[Deep Reinforcement Learning by Russ](https://deeplearning-cmu-10707.github.io/)

Building intelligent machines that are capable of extracting meaningful representations from high-dimensional data lies at the core of solving many AI related tasks. In the past few years, researchers across many different communities, from applied statistics to engineering, computer science and neuroscience, have developed deep (hierarchical) models -- models that are composed of several layers of nonlinear processing. An important property of these models is that they can learn useful representations by re-using and combining intermediate concepts, allowing these models to be successfully applied in a wide variety of domains, including visual object recognition, information retrieval, natural language processing, and speech perception. This is an advanced graduate course, designed for Masters and Ph.D. level students, and will assume a reasonable degree of mathematical maturity. The goal of this course is to introduce students to the recent and exciting developments of various deep learning methods.

[10-708 – Probabilistic Graphical Models](https://www.cs.cmu.edu/~epxing/Class/10708-20/)

Many of the problems in artificial intelligence, statistics, computer systems, computer vision, natural language processing, and computational biology, among many other fields, can be viewed as the search for a coherent global conclusion from local information. The probabilistic graphical models framework provides an unified view for this wide range of problems, enables efficient inference, decision-making and learning in problems with a very large number of attributes and huge datasets. This graduate-level course will provide you with a strong foundation for both applying graphical models to complex problems and for addressing core research topics in graphical models.

[10-725 - Convex Optimization](https://www.stat.cmu.edu/~ryantibs/convexopt/)

Nearly every problem in machine learning and computational statistics can be formulated
in terms of the optimization of some function, possibly under some set of constraints. As
we obviously cannot solve every problem in machine learning, this means that we cannot
generically solve every optimization problem (at least not efficiently). Fortunately, many
problems of interest in machine learning can be posed as optimization tasks that have
special properties—such as convexity, smoothness, sparsity, separability, etc.—permitting
standardized, efficient solution techniques
